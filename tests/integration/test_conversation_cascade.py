#!/usr/bin/env python3
"""
Simple test of the conversation system with model cascade fallback.

This script tests that the conversation system properly falls back
through the model cascade when rate limits are encountered.
"""

import sys
import os

# Add the app directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from app.core.models import ModelConfig
from app.services.openai_service import OpenAIService

def test_conversation_with_fallback():
    """Test a simple conversation that should trigger fallback."""
    print("ğŸ—£ï¸  Testing Conversation with Model Cascade")
    print("=" * 50)
    
    service = OpenAIService()
    
    # Show current cascade
    print(f"ğŸ”„ Model cascade: {' â†’ '.join(ModelConfig.FALLBACK_CASCADE)}")
    print(f"ğŸ“ Primary conversation model: {ModelConfig.CONVERSATION_MODEL}")
    
    # Test simple chat completion
    print("\nğŸ’¬ Testing simple chat completion:")
    try:
        response = service._safe_chat_create(
            model=ModelConfig.CONVERSATION_MODEL,
            messages=[
                {"role": "user", "content": "Say 'Hello from cascade test' and nothing else."}
            ],
            max_tokens=10,
            temperature=0
        )
        
        if response and response.choices:
            print(f"âœ… Success!")
            print(f"   ğŸ¤– Model used: {response.model}")
            print(f"   ğŸ’¬ Response: {response.choices[0].message.content.strip()}")
            if hasattr(response, 'usage') and response.usage:
                print(f"   ğŸ”¢ Tokens: {response.usage.total_tokens}")
                
            # Check if we used a fallback model
            if response.model != ModelConfig.CONVERSATION_MODEL:
                print(f"   âš ï¸  Note: Fell back from {ModelConfig.CONVERSATION_MODEL} to {response.model}")
            else:
                print(f"   âœ… Used primary model as expected")
        else:
            print("âŒ No response received")
            
    except Exception as e:
        print(f"âŒ Chat completion failed: {e}")
    
    # Show current rate limit status
    print(f"\nğŸ“Š Current rate limited models: {sorted(service._rate_limited_models)}")

def main():
    print("ğŸ§ª Conversation Cascade Test")
    print("=" * 30)
    
    try:
        test_conversation_with_fallback()
        print("\nğŸ‰ Test completed!")
        
    except Exception as e:
        print(f"\nğŸ’¥ Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()